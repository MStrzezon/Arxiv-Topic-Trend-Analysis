{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Doc2Vec"
   ],
   "metadata": {
    "id": "xK279Ul-2KZ-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reading Data from CSV"
   ],
   "metadata": {
    "id": "ZqVgUaIz2P4_"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DFSZf76xwPVj",
    "ExecuteTime": {
     "end_time": "2024-11-05T17:51:57.444448Z",
     "start_time": "2024-11-05T17:51:56.444562Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = \"../../data/arxiv.csv\"\n",
    "data = pd.read_csv(csv_file, sep=\";\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text cleaning\n",
    "1. Lowercasing - we convert all text to lowercase for uniformity,\n",
    "2. Remove punctuation - we eliminate unnecessary punkctuation marks\n",
    "3. Remove numbers - we are modelling topics from the text so numbers do not have any meaning in our case.\n",
    "4. Remove stopwords - stop words are very common words that carry no meaning or less meaning compared to other words. If we remove the words that are less commonly used, we can focus on the important words instead.\n",
    "6. Lemmatization - we extract the semantic root of a word (lemma) by considering the vocabulary and connections between the meanings of various words"
   ],
   "metadata": {
    "id": "TUBKnvgu598D"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ],
   "metadata": {
    "id": "pjUyyjqu57UA",
    "ExecuteTime": {
     "end_time": "2024-11-05T17:51:58.859651Z",
     "start_time": "2024-11-05T17:51:58.140669Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BszS91CU8Ux0",
    "outputId": "8b738dea-7cf6-4f3e-ea1b-f2117ea65a96",
    "ExecuteTime": {
     "end_time": "2024-11-05T17:52:00.262036Z",
     "start_time": "2024-11-05T17:52:00.031979Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mstrzezon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mstrzezon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/mstrzezon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/mstrzezon/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation, numbers and stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    return lemmatized_tokens"
   ],
   "metadata": {
    "id": "zGvlqCZ18WIo",
    "ExecuteTime": {
     "end_time": "2024-11-05T17:52:01.688190Z",
     "start_time": "2024-11-05T17:52:01.681084Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "data['Processed Summary'] = data.apply(lambda row: preprocess_text(row['Summary']), axis=1)",
   "metadata": {
    "id": "aGmxcEdz9JB8",
    "ExecuteTime": {
     "end_time": "2024-11-05T17:52:50.384900Z",
     "start_time": "2024-11-05T17:52:03.181884Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "data.head(10)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "nXml6X8X9Mga",
    "outputId": "4a1782ab-8e73-4678-90e4-62755955db63",
    "ExecuteTime": {
     "end_time": "2024-11-05T17:52:50.404844Z",
     "start_time": "2024-11-05T17:52:50.392114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                  ID  \\\n",
       "0  http://arxiv.org/abs/cs/0002002v1   \n",
       "1  http://arxiv.org/abs/cs/0002003v1   \n",
       "2  http://arxiv.org/abs/cs/0002009v1   \n",
       "3  http://arxiv.org/abs/cs/0003008v1   \n",
       "4  http://arxiv.org/abs/cs/0003016v1   \n",
       "5  http://arxiv.org/abs/cs/0003020v2   \n",
       "6  http://arxiv.org/abs/cs/0003021v1   \n",
       "7  http://arxiv.org/abs/cs/0003024v1   \n",
       "8  http://arxiv.org/abs/cs/0003028v1   \n",
       "9  http://arxiv.org/abs/cs/0003031v1   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Uniform semantic treatment of default and auto...   \n",
       "1           On the accuracy and running time of GSAT   \n",
       "2  Syntactic Autonomy: Why There is no Autonomy w...   \n",
       "3  Consistency Management of Normal Logic Program...   \n",
       "4  Abductive and Consistency-Based Diagnosis Revi...   \n",
       "5  ACLP: Integrating Abduction and Constraint Sol...   \n",
       "6  Relevance Sensitive Non-Monotonic Inference on...   \n",
       "7              A Compiler for Ordered Logic Programs   \n",
       "8           Logic Programs with Compiled Preferences   \n",
       "9                            Optimal Belief Revision   \n",
       "\n",
       "                                             Summary             Published  \\\n",
       "0  We revisit the issue of connections between tw...  2000-02-03T21:44:57Z   \n",
       "1  Randomized algorithms for deciding satisfiabil...  2000-02-04T12:53:57Z   \n",
       "2  Two different types of agency are discussed ba...  2000-02-16T18:09:20Z   \n",
       "3  This paper presents a method of computing a re...  2000-03-05T10:29:03Z   \n",
       "4  Diagnostic reasoning has been characterized lo...  2000-03-07T11:39:53Z   \n",
       "5  ACLP is a system which combines abductive reas...  2000-03-07T22:47:13Z   \n",
       "6  We present a method for relevance sensitive no...  2000-03-08T03:03:36Z   \n",
       "7  This paper describes a system, called PLP, for...  2000-03-08T10:15:51Z   \n",
       "8  We describe an approach for compiling preferen...  2000-03-08T14:09:56Z   \n",
       "9  We propose a new approach to belief revision t...  2000-03-08T15:54:50Z   \n",
       "\n",
       "                            PDF Link  Flesch reading ease  Number of words  \\\n",
       "0  http://arxiv.org/pdf/cs/0002002v1                26.30              195   \n",
       "1  http://arxiv.org/pdf/cs/0002003v1                35.47              188   \n",
       "2  http://arxiv.org/pdf/cs/0002009v1                22.85              160   \n",
       "3  http://arxiv.org/pdf/cs/0003008v1                45.69              127   \n",
       "4  http://arxiv.org/pdf/cs/0003016v1                17.30              110   \n",
       "5  http://arxiv.org/pdf/cs/0003020v2                21.33              171   \n",
       "6  http://arxiv.org/pdf/cs/0003021v1                 0.15              143   \n",
       "7  http://arxiv.org/pdf/cs/0003024v1                38.66              120   \n",
       "8  http://arxiv.org/pdf/cs/0003028v1                37.03              179   \n",
       "9  http://arxiv.org/pdf/cs/0003031v1                31.92              178   \n",
       "\n",
       "                                   Processed Summary  \n",
       "0  [revisit, issue, connection, two, leading, for...  \n",
       "1  [randomized, algorithm, deciding, satisfiabili...  \n",
       "2  [two, different, type, agency, discussed, base...  \n",
       "3  [paper, present, method, computing, revision, ...  \n",
       "4  [diagnostic, reasoning, characterized, logical...  \n",
       "5  [aclp, system, combine, abductive, reasoning, ...  \n",
       "6  [present, method, relevance, sensitive, infere...  \n",
       "7  [paper, describes, system, called, plp, compil...  \n",
       "8  [describe, approach, compiling, preference, lo...  \n",
       "9  [propose, new, approach, belief, revision, pro...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Published</th>\n",
       "      <th>PDF Link</th>\n",
       "      <th>Flesch reading ease</th>\n",
       "      <th>Number of words</th>\n",
       "      <th>Processed Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/cs/0002002v1</td>\n",
       "      <td>Uniform semantic treatment of default and auto...</td>\n",
       "      <td>We revisit the issue of connections between tw...</td>\n",
       "      <td>2000-02-03T21:44:57Z</td>\n",
       "      <td>http://arxiv.org/pdf/cs/0002002v1</td>\n",
       "      <td>26.30</td>\n",
       "      <td>195</td>\n",
       "      <td>[revisit, issue, connection, two, leading, for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/cs/0002003v1</td>\n",
       "      <td>On the accuracy and running time of GSAT</td>\n",
       "      <td>Randomized algorithms for deciding satisfiabil...</td>\n",
       "      <td>2000-02-04T12:53:57Z</td>\n",
       "      <td>http://arxiv.org/pdf/cs/0002003v1</td>\n",
       "      <td>35.47</td>\n",
       "      <td>188</td>\n",
       "      <td>[randomized, algorithm, deciding, satisfiabili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/cs/0002009v1</td>\n",
       "      <td>Syntactic Autonomy: Why There is no Autonomy w...</td>\n",
       "      <td>Two different types of agency are discussed ba...</td>\n",
       "      <td>2000-02-16T18:09:20Z</td>\n",
       "      <td>http://arxiv.org/pdf/cs/0002009v1</td>\n",
       "      <td>22.85</td>\n",
       "      <td>160</td>\n",
       "      <td>[two, different, type, agency, discussed, base...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/cs/0003008v1</td>\n",
       "      <td>Consistency Management of Normal Logic Program...</td>\n",
       "      <td>This paper presents a method of computing a re...</td>\n",
       "      <td>2000-03-05T10:29:03Z</td>\n",
       "      <td>http://arxiv.org/pdf/cs/0003008v1</td>\n",
       "      <td>45.69</td>\n",
       "      <td>127</td>\n",
       "      <td>[paper, present, method, computing, revision, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/cs/0003016v1</td>\n",
       "      <td>Abductive and Consistency-Based Diagnosis Revi...</td>\n",
       "      <td>Diagnostic reasoning has been characterized lo...</td>\n",
       "      <td>2000-03-07T11:39:53Z</td>\n",
       "      <td>http://arxiv.org/pdf/cs/0003016v1</td>\n",
       "      <td>17.30</td>\n",
       "      <td>110</td>\n",
       "      <td>[diagnostic, reasoning, characterized, logical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://arxiv.org/abs/cs/0003020v2</td>\n",
       "      <td>ACLP: Integrating Abduction and Constraint Sol...</td>\n",
       "      <td>ACLP is a system which combines abductive reas...</td>\n",
       "      <td>2000-03-07T22:47:13Z</td>\n",
       "      <td>http://arxiv.org/pdf/cs/0003020v2</td>\n",
       "      <td>21.33</td>\n",
       "      <td>171</td>\n",
       "      <td>[aclp, system, combine, abductive, reasoning, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://arxiv.org/abs/cs/0003021v1</td>\n",
       "      <td>Relevance Sensitive Non-Monotonic Inference on...</td>\n",
       "      <td>We present a method for relevance sensitive no...</td>\n",
       "      <td>2000-03-08T03:03:36Z</td>\n",
       "      <td>http://arxiv.org/pdf/cs/0003021v1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>143</td>\n",
       "      <td>[present, method, relevance, sensitive, infere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://arxiv.org/abs/cs/0003024v1</td>\n",
       "      <td>A Compiler for Ordered Logic Programs</td>\n",
       "      <td>This paper describes a system, called PLP, for...</td>\n",
       "      <td>2000-03-08T10:15:51Z</td>\n",
       "      <td>http://arxiv.org/pdf/cs/0003024v1</td>\n",
       "      <td>38.66</td>\n",
       "      <td>120</td>\n",
       "      <td>[paper, describes, system, called, plp, compil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://arxiv.org/abs/cs/0003028v1</td>\n",
       "      <td>Logic Programs with Compiled Preferences</td>\n",
       "      <td>We describe an approach for compiling preferen...</td>\n",
       "      <td>2000-03-08T14:09:56Z</td>\n",
       "      <td>http://arxiv.org/pdf/cs/0003028v1</td>\n",
       "      <td>37.03</td>\n",
       "      <td>179</td>\n",
       "      <td>[describe, approach, compiling, preference, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://arxiv.org/abs/cs/0003031v1</td>\n",
       "      <td>Optimal Belief Revision</td>\n",
       "      <td>We propose a new approach to belief revision t...</td>\n",
       "      <td>2000-03-08T15:54:50Z</td>\n",
       "      <td>http://arxiv.org/pdf/cs/0003031v1</td>\n",
       "      <td>31.92</td>\n",
       "      <td>178</td>\n",
       "      <td>[propose, new, approach, belief, revision, pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save the processed text"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T17:18:50.914747Z",
     "start_time": "2024-11-05T17:18:49.030532Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 18,
   "source": "data.to_csv(\"../../data/arxiv_processed.csv\", sep=\";\", index=False)"
  }
 ]
}
